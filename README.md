# ELT PIPELINE PROJECT
## **Designing an ELT Pipeline and Implementing SCD Strategy in a Data Warehouse Using Luigi and dbt**

This project is focus on designing the ELT pipeline for datawarehouse with implementing the SCD strategies using. This project uses the "Kimball Method" approach to design the data warehouse model.

Before we jump in to the step of designing the data warehouse, first we need understand the bussiness process. Imagine you are a data engineer, you have a client called "pactravel" that manage application for online bookings for airline & hotel ticket. The picture shown bellow is the ERD from database of pactravel
![pactravel - erd](https://github.com/user-attachments/assets/d7e6d136-7e30-4299-ad06-818a3cfdd020)

 Hereâ€™s a breakdown of the business process based on the ERD:
 1. Customer Registration
    - Customers register their details in the system, creating a profile with personal and contact information.
 
 2. Flight Booking:
    - Customers can book flights by selecting an airline and flight details.
    - The system records the flight number, airline, aircraft, and airport information.
    - Pricing and class are also selected during booking.
 
 3. Hotel Booking:
    - Customers can book hotels by choosing a hotel and specifying check-in/check-out dates.
    - Pricing and additional services (e.g., breakfast) are recorded.

 4. Integration:
    - The system integrates flight and hotel bookings, allowing seamless travel planning.
    - Customers can manage their bookings through the system.

## 1. Requirement Gathering
Based on the database structure and business context, here are the potential problems and answer that might Pactravel needs for designing the data warehouse:
### PROBLEMS AND STAKEHOLDER NEEDS

**Problem**
The management Difficult to track revenue  generated by single customer of product purchasing on booking flights or hotels. The management also want to track customer preferences on booking flights or hotels, this data will drive the management to observe the customer behavior to increase effectivity of pricing strategy.

**Solution**
Build the dashboard to track sales by customer and their booking preferences for booking flights or hotels. 

## 2. Select the Businsess Process & Declare the Grain
From the previous problems identified:
- Booking Flights Process Order
- Booking Hotels Process Order


Here are the relevant business processes and the grain that should be modeled:

**1. Booking Flights Process Order:**
Grains: 
- A single record represents one flight booking transaction per customer, capture detail about price, airline name, and travel class. 


**2. Booking Hotels Process Order:**
Grains:
- A single record represents one hotel booking transaction per customer, capture detail about price, hotel name, duration, and breakfast preferences.


 
 ## Identify the Dimension
 After the grain has been defined, the next step is to identify the dimension needed for the data warehouse. The dimension needed for data warehouse include:



1. Aircraft Dimension (dim_aircraft)
   Store information about various type of  aircraft that used in flight

2. Airlines Dimension (dim_airlines)
   Store information about the airlines operating the flights

3. Airport Dimension (dim_airport)
   Stores information about airport and location

4. Customer Dimension (dim_customer)
   Stores detail information about costumer who used the pactravel application.

5. Hotel Dimension (dim_hotel)
   Contain hotel property information

## Identify the Fact
After we define the dimension, the next step is to identify the fact table where this fact table will store any measurement that will answer the question of the stake holders, the fact table include:

1. flight booking fact table (fact_flight_booking)
   This table contain information about the detail of the booking flight from single customer_id
   
2. hotel booking fact table (fact_hotel_booking)
   This table contain information about the detail of the booking hotel from single customer_id
   

Bellow is the dimension table detail:

# Dimension Tables

## A. dim_aircrafts

| Column Name    | Data Type | Description   |
|----------------|-----------|---------------|
| aircraft_sk    | UUID      | Surrogate Key |
| aircraft_nk    | VARCHAR   | Natural Key   |
| aircraft_iata  | VARCHAR   | IATA Code     |
| aircraft_icao  | VARCHAR   | ICAO Code     |

## B. dim_airlines

| Column Name   | Data Type | Description   |
|---------------|-----------|---------------|
| airline_sk    | UUID      | Surrogate Key |
| airline_nk    | INT       | Natural Key   |
| airline_name  | VARCHAR   | Airline Name  |
| country       | VARCHAR   | Country       |


## C. dim_airports

| Column Name   | Data Type | Description   |
|---------------|-----------|---------------|
| airport_sk    | UUID      | Surrogate Key |
| airport_nk    | INT       | Natural Key   |
| airport_name  | VARCHAR   | Airport Name  |
| airport_city  | VARCHAR   | City          |

## D. dim_customers

| Column Name         | Data Type | Description   |
|---------------------|-----------|---------------|
| customer_sk         | UUID      | Surrogate Key |
| customer_nk         | INT       | Natural Key   |
| customer_first_name | VARCHAR   | First Name    |
| customer_family_name| VARCHAR   | Family Name   |
| customer_gender     | VARCHAR   | Gender        |
| customer_country    | VARCHAR   | Country       |

## E. dim_hotels

| Column Name   | Data Type | Description   |
|---------------|-----------|---------------|
| hotel_sk      | UUID      | Surrogate Key |
| hotel_nk      | INT       | Natural Key   |
| hotel_name    | VARCHAR   | Hotel Name    |
| hotel_city    | VARCHAR   | City          |
| hotel_country | VARCHAR   | Country       |
| hotel_score   | FLOAT     | Score         |

# Fact Table

## A. fact_booking_flights

| Column Name   | Data Type | Description   |
|---------------|-----------|---------------|
| fact_booking_flights_sk     | UUID | Surrogate Key |
| trip_id_nk      | INT       | Natural Key    |
| flight_number_nk      | INT       | Natural Key    |
| seat_number_nk      | INT       | Natural Key    |
| customer_sk     | INT   | Foreign Key     |
| airline_SK      | INT   | Foreign Key          |
| travel_class    | VARCHAR   | Class of Seat in Flights       |
| price           | INTEGER   | Price of Flights   |

## B. fact_booking_hotels

| Column Name   | Data Type | Description   |
|---------------|-----------|---------------|
| fact_booking_hotel_sk      | UUID      | Surrogate Key |
| trip_id_nk      | INT       | Natural Key   |
| hotel_sk    | INT   | Foreign Key    |
| customer_sk    | INT   | Foreign Key          |
| breakfast_included | BOOL   | Foreign Key       |
| duration   | INT     | Total Days of stay at hotel|

## Bus Matrix
![Screenshot 2024-11-09 215753](https://github.com/user-attachments/assets/79f971c0-36ed-4b3c-87e8-36b60038dc50)

# Determine the SCD Strategies
Based on the information gathered from the client, we can determine the appropriate Slowly Changing Dimension (SCD) strategies for the pactravel data warehouse model. Let's break it down by dimension:

A. dim_aircrafts
 The data on dim_aircrafts typically don't change often and if they do , we usually want to see the most current state. The SCD Type 1 will be implemented on dim_aircrafts.

B. dim_airports
  The information that stored in dim airports is rarely change, so this dimension will implemented SCD type 1

C. dim_airlines
  Same like dim_aircraft and dim_airline, the data on dim_airlines is typically don't change often. but it's still need to track the current state. this dimension will implenting the SCD type 1

D. dim_customers
 The dim_customers will implemented the SCD type 2, because it will track the change of customers data like the change of address, emailor phone numbers.

E. dim_hotels
 The dim_hotels need to implemeted SCD type 2, because the rating of the hotel in dim_hotels will oftenly change. In somecase, maybe the name of the hotel will change to regarding the management issue.

![Screenshot 2024-11-09 215753](https://github.com/user-attachments/assets/e868b421-cacf-4174-b7c0-52fa21592156)

here the final ERD of the datawarehouse:

![Screenshot 2024-11-16 231424](https://github.com/user-attachments/assets/6e9677e8-2673-4ea5-be9d-c96bd008b265)

## Implementation Details

- For Type 2 SCD:
  - Add columns: `effective_date`, `end_date`, `is_current`
  - Insert new row for changes, update dates and flags accordingly

- For Type 1 SCD:
  - Simply update the existing row when changes occur

# ELT WORKFLOW
In this project, we will use 2 database. one database for the source of the data, and the other database for build the datawarehouse. we will copy all of the the data from the source database to target database. on target database, will consist of 2 schema, pactravel and final. pactravel will store all information from sources, and the final will be the schema for our datawarehouse. On final schema, the data will be transformed. Here the work flow:

![Screenshot 2024-11-16 231424](https://github.com/user-attachments/assets/24dc58b1-88fb-4e64-b287-07efd790b4a1)

# ELT PIPELINE ORCHESTRATION
In this project, we will use luigi as the orchestration tools. We will extract all the data from the database as the csv file format, and then dump the data to the pactravel schema on the target database where we will implemented the datawarehouse schema. As the data from pactravel schema is ready, we will use dbt as the tool for transform the data and dump it to the final schema. Sentry will be used as alerting and notification tools. 

![FIGURE](https://github.com/user-attachments/assets/4896131f-d994-4504-a385-09afc3038c2c)

Tools:
- Orchestration: Luigi
- Schedulling: Cron
- Write summary : pandas
- Logging: logging.info for write logs for every step of the proccess and logging.error for write error
- alerting & notification: sentry. you can visit the sentry website to read the documentation in this [link](https://sentry.io/welcome/)

 How to use this repo:
## 1. Requirements
- **OS:**
  - Linux
  - WSL
- **Tools:**
  - Dbeaver
  - Docker
  - Cron
  - dbt
- **Programming Language:**
  - Python
  - SQL
- **Python Library:**
  - Luigi
  - Pandas
  - Sentry-SDK
- **Platforms**
   - Sentry

## 2.Preparations
- Clone this repo using the following command:

  ```bash
  git lfs clone https://github.com/ibnufajar1994/elt-pipeline-project.git
  ```
run the command below on the terminal:
  ```bash
  docker compose up -d
  ```
  
- Create Sentry Project
  - visit: https://www.sentry.io
  - Signup with email that you want get notifications and alert
  - Create Project on sentry:
    - use python as a platform
    - set alert frequency as "on every new issue"
    - Create project name
    - **Copy the DSN of your project into .env file**

- Create temp dir. on your root project directory:
```bash
mkdir pipeline/temp/data
mkdir pipeline/temp/log
```
- Create & use virtual environment on your root directory project
- Install the requirements using the following command:
```bash
pip install -r requirements.txt
```

- **Create env file in your root project directory, copy this variable into it.you need to adjust the value based on your preferences:**
  
```bash
SRC_POSTGRES_DB=...
SRC_POSTGRES_HOST=...
SRC_POSTGRES_USER=...
SRC_POSTGRES_PASSWORD=...
SRC_POSTGRES_PORT=...

# DWH
DWH_POSTGRES_DB=...
DWH_POSTGRES_HOST=...
DWH_POSTGRES_USER=...
DWH_POSTGRES_PASSWORD=...
DWH_POSTGRES_PORT=...

# SENTRY DSN
SENTRY_DSN=... # Fill with your Sentry DSN Project 

# DIRECTORY
# Adjust with your directory. make sure to write full path
DIR_ROOT_PROJECT=...     # <project_dir>
DIR_TEMP_LOG=...         # <project_dir>/pipeline/temp/log
DIR_TEMP_DATA=...        # <project_dir>/pipeline/temp/data
DIR_EXTRACT_QUERY=...    # <project_dir>/pipeline/src_query/extract
DIR_LOAD_QUERY=...       # <project_dir>/pipeline/src_query/load
DIR_DBT_TRANSFORM=...    # <project_dir>/dbt_transform/
DIR_LOG=...              # <project_dir>/logs/
```
- run this command on the backround process:
  ```bash
  luigid --port 8082 &
  ```
- you can run this command directly on the terminal to run the pipeline:
   ```bash
   python3 elt_main.py
   ```
- or you can schedulling using cron, for example on the code below is the command to run the pipeline every one hour.
    ```bash
   0 * * * * <project_dir>/elt_run.sh
   ```


